{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "trzq2dAHg8Qb"
   },
   "outputs": [],
   "source": [
    "from helpers import clean_text, create_encoder, load_dataset\n",
    "from helpers import create_tokenizer, max_length, encode_text\n",
    "from helpers import encode_label, define_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eec9xyDdg8Ks",
    "outputId": "1de70c30-0c61-4334-9a8e-97f109bf960e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 67\n",
      "Vocabulary size: 227194\n",
      "(141463, 67)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# load training dataset\n",
    "tweets = load_dataset('/content/drive/MyDrive/sampled_tweet.pkl')\n",
    "tweets['tweet'] = tweets.tweet.apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets.tweet, tweets.dialect, \n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = 42)\n",
    "trainLines, trainLabels = X_train, y_train\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)\n",
    "\n",
    "#create label encoder\n",
    "encoder = create_encoder(y_train)\n",
    "# encode labels \n",
    "trainY = encode_label(encoder, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t--RVu0hCxx",
    "outputId": "22fc285f-107d-4d1c-c091-d1b72d33f143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 67)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 67, 100)           22719400  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 64, 32)            12832     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 32)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 32, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                10250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,742,680\n",
      "Trainable params: 22,742,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "1879/1879 [==============================] - 64s 29ms/step - loss: 2.4152 - accuracy: 0.2231 - val_loss: 2.0015 - val_accuracy: 0.3782\n",
      "Epoch 2/10\n",
      "1879/1879 [==============================] - 57s 30ms/step - loss: 1.4940 - accuracy: 0.5289 - val_loss: 1.7559 - val_accuracy: 0.4761\n",
      "Epoch 3/10\n",
      "1879/1879 [==============================] - 53s 28ms/step - loss: 0.8481 - accuracy: 0.7373 - val_loss: 1.8581 - val_accuracy: 0.5037\n",
      "Epoch 4/10\n",
      "1879/1879 [==============================] - 53s 28ms/step - loss: 0.5185 - accuracy: 0.8397 - val_loss: 2.1404 - val_accuracy: 0.5066\n",
      "Epoch 5/10\n",
      "1879/1879 [==============================] - 53s 28ms/step - loss: 0.3482 - accuracy: 0.8915 - val_loss: 2.4725 - val_accuracy: 0.5115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49e3c47050>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model, callback = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit(trainX, trainY, \n",
    "          epochs=10, batch_size=64, \n",
    "          validation_split= 0.15,\n",
    "          callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fr7NPyMukNmf"
   },
   "outputs": [],
   "source": [
    "# encode data\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "testY =  encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GgfaH6h4kNbF"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(testX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvlIvlqhNpcI",
    "outputId": "2e3155aa-3ab9-4fe6-bfd9-238b59ad63a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is 51.13%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score\n",
    "pred = np.argmax(prediction, axis=1) \n",
    "y_pred = encoder.inverse_transform(pred)\n",
    "print(\"Testing accuracy is {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nHn2xrpejhPR"
   },
   "outputs": [],
   "source": [
    "model.save('CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9qpjs6ujtzl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iKkI8Or6gz9h",
    "fjAebbhDODwo",
    "D6eXqZjcBfHF"
   ],
   "name": "Dialect_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
